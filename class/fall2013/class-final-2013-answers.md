---
layout: page
title: Take-home Final, Fall 2013
header_image: about-nyu-washingtonsquare.jpg
---


## Introduction

This page contains the answers to the final for my NYU-SCPS Data Journalism class for Fall 2013; [you can see the final sans answers here](http://www.smalldatajournalism.com/class/fall2013/class-final-2013/). The final was take-home and short-answer, though a few students demonstrated the answers with spreadsheets/visualizations of their own. 

My comments and answers are marked in <span style="color: #707;">orange</span> and where possible, I try to link back to [reading material](/readings) and examples we discussed in [class](http://www.smalldatajournalism.com/class/) to justify the answers.



## Analysis

Correlation does not equal causation. In each of the (fictional) examples below, a plausible assertion is made **in bold**. However, each of these claims are suspect.

**For each of these claims**, either describe a flaw in the statistical reasoning, or describe at least one *additional* source information you would need to verify the claim. 

**Example assertion:** 

In the city of Springfield, hospitals devote an average of 6% of their operating budgets on their cardiac surgery departments. Springfield Municipal, however, devotes 15% of its budget to cardiac surgery.

Yet despite the increased funding, Springfield Municipal averages 7 deaths per 1,000 open-heart surgeries, compared to a rate of 2.3 among other Springfield hospitals. **Clearly, Springfield Municipal is misspending its cardiac funds.**

**Example answers**

1. Just because Springfield Municipal is spending more than twice of its budget *proportionally* on heart surgery doesn't mean that it is spending *more* money overall. If its budget is $10 million, 15% of that is $1.5 million. However, if the average hospital has a budget of $100 million and thus spends $6 million on cardiac surgery, then the average hospital actually spends 4 times as much as Springfield Municipal in absolute terms.
2. The mortality rate says nothing about the health of the patients that Springfield Municipal sees. If Municipal is considered to have the best cardiac surgery team in the city, then it may also be the hospital in which highest risk cardiac patients are sent. Therefore, we need to know the expected mortality rate of cardiac patients for each hospital in order to make an apples-to-apples comparison.


**Now respond to the following assertions:**
---------------------

\1. For every 100,000 people, at least 48 die annually from knife-related accidents. In comparison, only 0.5 deaths per 100,000 people can be attributed to hand grenades. **Therefore, hand grenades are nearly 100 times safer than knives.**

<span class="my-notes">
Beware of the authorative-sounding nature of percentages, just because a claim involves a _rate_ doesn't mean that it is more of an accurate or scientific conclusion.
</span>

<span class="my-notes">
It's not the rate _alone_ that is important, but the _population_ from which the grenade and knife death rates are derived. Are we talking about 0.5 deaths for every 100,000 _grenade owners_? Or just 100,000 in the _general population_? Because if it is the latter, then the comparison of death rate is nearly useless. 

Assuming that in a general (peacetime) population, there are far more knife owners than grenade owners, then the fact that grenade deaths are very rare could mean that just very few people are actually around grenades. If more people owned (and used) grenades in everyday life, the rates here are likely to change.
</span>    

<span class="my-notes">
There's an excellent real-life example of this in the [New York Times investigation into child gun  accidents](http://www.nytimes.com/2013/09/29/us/children-and-guns-the-hidden-toll.html?). A gun rights group is quoted as stating, "that children are '130 percent more likely to die from choking on their dinner' than from accidental shootings."

That may be so. But nearly 100% of children in America eat dinner every day of their lives. Whereas a smaller percentage of children handle guns even on a weekly basis. 
</span>



\2. In a random sampling of deaths among 18 to 40-year olds who consumed an average of 3 alcoholic beverages a week, nearly 35 percent of the deaths were alcohol-related, e.g. accidents and liver disease. However, in a sampling of deaths among 80 to 100-year olds who consumed at least 5 alcoholic beverages a week, only 20 percent of the deaths were alcohol-related. **Therefore, we can conclude that drinking more alcohol will reduce your chances of having an alcohol-related death.**

<span class="my-notes">

This is an example of [survivor bias](http://freakonomics.com/2009/09/17/survivor-bias-on-the-gridiron/), in a very literal sense. People who live to be more than 80 years old are a rare subset of the general population, so it raises the question: how did they manage to live that long? High personal wealth, strong social support systems, and good genetics play a part in that. And for our particular scenario, how the _heck_ did they live to be 80+ even after drinking all that alcohol (assuming they didn't take up drinking late in life)?

We don't know for sure, and it would help to know the size of the population here. Perhaps most people in this 80+ year old group also have that rare invincible liver-gene. Or, more likely, if you've been averaging several alcoholic drinks a day for more than 60 years, then you can be said to have successfully avoided the bad behaviors that typically cause alcohol-related deaths.

On the other hand, if you live in an industrialized nation, dying before 40 is a rare occasion. And those who die young from causes related to alcohol consumption &ndash; such as alcohol poisoning or drunk driving &ndash; did not have the same luck or discretion as those in the elderly drinking club. One of my students pointed another factor: younger people may also have more exposure to risky drinking activities, such as pairing alcohol with other drugs, than the average elderly person. 

And of course, there is plenty of plain sample bias here. The top causes of death are distinctly different between the young and the elderly.  From the ages of 1 to 44 in the U.S., the leading cause of death is "Unintentional injury", according to the [Centers for Disease Control and Prevention](http://www.cdc.gov/injury/wisqars/pdf/10LCID_All_Deaths_By_Age_Group_2010-a.pdf), whereas from 65+ onwards, heart disease is the dominant killer.

So even if there's no difference in risky drinking behavior between the young and the elderly, the fact that most elderly succumb to heart disease may be enough to reduce the percentage of their alcohol-related deaths.
</span>


\3. When a movie franchise has *at least one sequel*, the franchise *averages* $25 million more in revenue **per movie** than the average individual movie. **Therefore, given an original screenplay, a movie studio should prepare to film a sequel as soon as possible, in order to maximize revenue**. 


<span class="my-notes">

This is just confusing cause with effect. Why are sequels made? Because the original movie made a lot of money. [Even movies that necessitate a sequel won't get one if the first bombed](http://en.wikipedia.org/wiki/The_Golden_Compass_(film)#Sequels).

While there are standalone movies that do well ("Titanic" comes to mind) without spawning a sequel, almost all first-in-a-series movies have performed well. So it's a good assumption that they will also have respectiable revenues on average, at least compared to any given standalone movie.
</span>



\4. From the year 2008 to 2012, SAT scores among Springfield High seniors jumped 38%. Shelbyville High seniors, however, had a 1% drop in overall SAT scores. **Therefore, we can conclude that Springfield High graduates will have a better chance of going to a prestigious college.** 

<span class="my-notes">
Note: I probably should've stated that we're dealing with _averages_ here, not that it makes a real difference in this situation.

This is an example where percentages _alone_ don't provide enough context. If Springfield High's average SAT score in 2008 was 1000, then a 38% increase moves that average to 1380. However, if Shelbyville started at an average of 2000, then a 1% drop still leaves them at a 1980. I'll leave aside the notion that better SAT scores translates into a "better chance" at college admissions, but if that's the only criteria we have, then Shelbyville still comes out on top.

In general, whenever a claim sticks solely to percentages, without once mentioning the actual absolute numbers, you should immediately suspect that it's hiding something. A common scenario is when huge percentages are trumpeted &ndash; _Our book sales jumped 400% from last year_ &ndash; to gloss over the less impressive underlying numbers &ndash; _We sold $40 worth of books this year compared to $10 last year_.
</span>


\5. In the year 2000, the suburb Deerfield had a deer population that averaged 2.4 per acre. In that year, there were only 3,800 concealed-carry firearm permits on file for Deerfield residents. In 2010, however, the deer population fell to an average of 0.6 an acre, whereas concealed-carry firearm permits increased to more than 40,000. **From this, we can conclude that Deerfield gun ownership is to blame for the decrease in deer population.**

<span class="my-notes">
Guns kill deer. If there were less deer and more guns than before, then it stands to reason that the guns are the cause of the drop in deer population.

I deliberately chose the metric of concealed-carry (CCW) permits over something more obvious, such as hunting licenses or deer tags sold. Generally, deer hunting is not done with the kind of handguns associated with CCW permits. So one objection to this claim is that even if guns are to blame, this is the wrong metric to look at.

However, that's not the _main_ problem, as I see it. I think CCW permits is an OK proxy for hunting licenses. People who

</span>


## Data Mashing

Below are the table structures (and some example data) from two sources of information: the city restaurant inspections department and 311 reports. Pretend that the tables all include thousands of rows instead of just the few example rows I've listed.


#### Restaurant inspections



**Restaurant listings**

<table class="table table-striped table-bordered"><thead><tr><th>Name</th><th>Address</th><th>Zip</th><th>Category</th></tr></thead><tbody><tr><td>Bob's Burgers</td><td>98 Broadway</td><td>10004</td><td>Burgers</td></tr><tr><td>Big Kahuna Pizza</td><td>1200 6th Ave</td><td>11412</td><td>Pizza</td></tr></tbody></table>

**Inspection reports**

<table class="table table-striped table-bordered"><thead><tr><th>Inspection date</th><th>Restaurant name</th><th>Health grade</th></tr></thead><tbody><tr><td>10/14/10</td><td>Bob's Burgers</td><td>B</td></tr><tr><td>8/3/10</td><td>Baconland</td><td>C</td></tr></tbody></table>

**Violations noted**

<table class="table table-striped table-bordered"><thead><tr><th>Violation type</th><th>Inspection date</th><th>Restaurant name</th></tr></thead><tbody><tr><td>Unwashed utensils</td><td>8/3/10</td><td>Baconland</td></tr><tr><td>Rodent droppings</td><td>8/3/10</td><td>Baconland</td></tr><tr><td>Refrigerator not cold enough</td><td>10/14/10</td><td>Bob's Burgers</td></tr></tbody></table>


#### 311 Calls that involved rodent sightings

<table class="table table-striped table-bordered"><thead><tr><th>Intersection</th><th>Call Date</th><th>Rats sighted</th><th>Lat</th><th>Long</th></tr></thead><tbody><tr><td>3rd Ave and 14th Street</td><td>5/2/12</td><td>5</td><td>-37.28237</td><td>83.1234</td></tr><tr><td>Broadway and Jane St.</td><td>3/3/12</td><td>2</td><td>-37.10293</td><td>85.12392139</td></tr></tbody></table>



**Explain the steps you would take to merge the four tables to see if there's a correlation between rat sightings and rodent-related violations noted by restaurant inspectors.**

In other words, I want to know: "Is the number of 311 rat sightings a good indicator of whether nearby restaurants will have a high number of rodent-related health violations?"

How do I get a dataset that would help answer that question?

**Hint:** In the case where you don't have an exact foreign key to use between tables, assume that you're able to also derive geographical coordinates through some third-party program and that you can link datasets by how close they are to a given set of coordinates.

Here's one of the steps:

> In both the violations and inspections tables, create a new field that consists of the **inspection date** and **restaurant name** concatenated together. Then **merge** the inspections and violations table on that new field.


**Now describe the rest of the steps.**

<span class="my-notes">

The point of this awkwardly worded exercise is to test the understanding of how different datasets can be joined together. Technically, there are __two__ different data __sources__: the restaurant inspections and the 311 reports.  However, the restaurant inspections consists of three tables:

1. __Restaurant listings__ - the names and addresses of establishments.
2. __Inspection reports__ - the names of restaurants, and the grades and dates of their inspections.
3. __Violations noted__ - For each inspection, there can be more than one violation found.

So let's start with doing the __two__ joins needed to merge these three tables together:

1. The __foreign key__ between __restaurant listings__ and __reports__ is straightforward: Use the __restaurant name__ to connect each establishment to the inspections it received.
2. While it is possible to join __violations__ directly to the __restaurant listings__ using the __name__ field , it makes more sense to join to the __inspections__ table first. We do this by concatenating the __inspection date__ and __restaurant __name__ fields in __violations__ and __inspections__ to serve as the __foreign key__ joining each violation to each inspection.

The resulting merged restaurant inspections will look like this:



<table class="table table-striped table-bordered"><thead><tr><th>Name</th><th>Address</th><th>Zip</th><th>Category</th><th>Health grade</th><th>Inspection date</th><th>Violation type</th></tr></thead><tbody><tr><td>Bob's Burgers</td><td>98 Broadway</td><td>10004</td><td>Burgers</td><td>B</td><td>10/14/2010</td><td>Refrigerator not cold enough</td></tr><tr><td>Bob's Burgers</td><td>98 Broadway</td><td>10004</td><td>Burgers</td><td>B</td><td>10/14/2010</td><td>Rodent droppings</td></tr><tr><td>Bob's Burgers</td><td>98 Broadway</td><td>10004</td><td>Burgers</td><td>C</td><td>4/14/2011</td><td>Rodent droppings</td></tr></tbody></table>

By joining the tables, you end up having a row for every combination of restaurant and associated violation -- in the above example, I've added a couple of more hypothetical violations for "Bob's Burgers" to illustrate this __one-to-many__ relationship, i.e. each __restaurant__ has many __violations__. The fact that there's a lot of redundant data (e.g. the repeated addresses) here is just a little messiness we put up with for not using proper database software.

__One note:__ Actually, you can just join the __restaurant listings__ directly to the __violations__ table, using the __name__ field, unless you care about the actual health grade, which we don't, for the purposes of this exercise.


OK, now we join our merged table to the __311 reports__, which consists of a single table. But there's no obvious key between them. The 311 data just shows intersections, not exact addresses nor name of establishment.


So the concept here is that data can be joined through _implied_, if not _exact_ relationships. The __intersections__ field is too varied to directly link it to restaurant addresses. However, __latitude__ and __longitude__, which is an unambiguous representation of location, is something we can work with.

__We just have to get the latitude and longitude for each of the restaurant addresses__. Then we can associate 311 reports to restaurant locations via the [distance formula](https://www.khanacademy.org/math/algebra/linear-equations-and-inequalitie/more-analytic-geometry/v/distance-formula) (remember high school geometry class?)

In this class, we didn't cover database query language or the kind of scripting that could easily generate these derived fields, so it's enough to explain it conceptually:

> We create a new table of 311 reports and restaurant inspection data by associating every rodent report to a restaurant inspection __if the restaurant and 311 report are within 200 meeters of each other__

One more detail: we aren't simply correlating the geographical proximity of inspections and rodent sightings, but also the __chronological__ timing. This is where the __inspection date__ is important to have, so that you can see if,__ within a certain number of weeks of a 311 rodent sighting__, nearby restaurants were also found to have rodent problems?

It's important to realize that just because you have the data in an analyzable form doesn't mean that the analysis will be _sound_. For one thing, 311 reports are voluntary. Just because no one called in a rodent sighting doesn't mean there aren't rodents nearby. And places with a lot of 311 rodent reports may just be places where there happen to be a lot of watchful citizens. And of course, restaurant inspections are subject to human error.

But testing those assumptions requires getting the data into shape _in the first place_, and that is its own problem-solving task... 
</span>





## Visualization

I want to create a visualization so that New York educators can more easily determine which schools need the most help in SAT Math prep.

So I've created two maps. The first shows absolute SAT Math scores for each school in 2010:

![image](/images/class/2013-10-28/SAT-2010.jpg)


The second map shows Math SAT scores in 2012:

![image](/images/class/2013-10-28/SAT-2012.jpg)



You can see the interactive versions of the maps in these links: [2010](https://www.google.com/fusiontables/DataSource?docid=104fGFiIH8MgjgQ41FhfYiuBOSv3c2uGtSM_eZZ4#map:id=5) and [2012](https://www.google.com/fusiontables/DataSource?docid=104fGFiIH8MgjgQ41FhfYiuBOSv3c2uGtSM_eZZ4#map:id=6)



\1. Describe how these maps might make the data *harder* to comprehend.

The answer to this can basically be found in Matt Ericson's "[When Maps Shouldn't Be Maps](http://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/)". In musing whether a map overlaying Katrina flood damage and resident income, he writes:

> But while maps like that are interesting to look at, it also forces readers who want to figure out the correlation between income and flooding to try and visually sum up all the colors on the map in their head. The map shows there’s low-income areas in the flooded areas and there’s also low-income areas outside the flooded areas. There’s middle- and upper-income areas in each, too. Unless the pattern is super clearcut, trying to figure out how much of a relationship exists is a tricky task.


In my crude maps of SAT scores, I'm not even trying to show correlation between two data points, but just a simple comparison: each school's 2010 and 2012 average SAT math score. But to see if a school improved or declined, you not only have to look from one map to the other, but in that transition, you have to keep track of which dot was which color, and whether you're even looking at the correct dot.

99% of the ink is devoted to showing __geographic data__ because, well, _that's what maps do_. And yet what we ostensibly care about is math scores per school, independent of where those schools are physically located.

You may argue that it's worth looking at neighborhood income levels and finding a correlation between school SAT scores, but that's a different problem. For the question at hand &ndash; _which_ (not _why_) schools need better math instruction &ndash; a map provides more distraction than illumination.

And to compound the problem, I've used 5 different colors, or "buckets", to divvy up the school performances, which only adds to the visual confusion.



\2. Propose a different visualization that would more effectively make my point.

Just about any of the simple visualization types, such as bar graphs, would be better. Or how about a simple table?

TK IMG

Now the interested reader can skim the table to see which schools are on a downward swing. Also, when it comes to SAT scores, it's not just _percentage_ of change that matters, but the actual scores themselves. A school that averages at 100 and improves to 200 is, by most educator's standards, a school that still requires math instruction, though the large change might help policymakers investigate positive factors that can be reinforced.

\3. If this data *must* be shown as a map, how can its visualization styles be changed to at least make it less confusing?

Summarize the data so that it can be shown on __one_ map and reduce the number of "buckets". This map could consist solely of dots colored red, green, or yellow, with the colors representing the difference between 2012 and 2010. This at least eliminates the difficulty of comparing two maps against each other:

TKMAP img

https://www.google.com/fusiontables/DataSource?docid=104fGFiIH8MgjgQ41FhfYiuBOSv3c2uGtSM_eZZ4#map:id=4


__More thoughts on maps:__ In my class, most of our visualization work involved creating maps. However, this was not because maps were ideal, but because they were so easy to create once you figured out Google Fusion Tables. We didn't have enough time to cover basic data journalism concepts, data-joining/cleaning techniques, _and_ creating visualizations, so resorting to making maps of our datasets was a compromise.

However, this is the state of web visualization in general. Creating interactive maps is so easy that once your dataset includes address data, _why not_ make a map out of it? Anecdotally, even if tables are clearer in presenting your analysis, maps always seem to grab more attention.

For example, the New York Times' default view for its election results are maps. [Here's the page for the 2012 Senate races](http://elections.nytimes.com/2012/results/senate):





The map is beautifully done, but look how much of it is wasted on non-existent data: the gray states in which no Senate race occurred. And even in the states that did have Senate races, does the map tell an interested viewer anything that isn't already obvious? Texas, Utah, Mississippi, et. al now have Republican senators. New York and California have Democratic senators. To find out any more about the races, you have to manually hover over each state.

Now [check out the "Big Board" view of the Senate race](http://elections.nytimes.com/2012/results/senate/big-board):


There's no "gray" ink here, every datapoint matters. Even more importantly, the results are given important context: which races were deemed tossups, and which party won those tossups. Winning or retaining the majority is an absolute game-changer for a political party, and the Big Board gets to that point with a minimum amount of fuss, whereas the nationwide map spends most of its ink reminding you where U.S. states are geographically located.

But if you were to glance at the map versus the Big Board, which is immediately more pleasing and seemingly engaging? Or, to put it another way, if you were a designer, which of the two visualizations would you place first on your design portfolio?

So the question of "which visualization is always better?" usually doesn't have an easy obvious answer. The Times' Matt Ericson explores the tradeoffs in map-based visualizations in his 2011 post, [When Maps Shouldn’t Be Maps](http://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/)








